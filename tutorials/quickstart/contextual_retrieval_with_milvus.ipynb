{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "uXP8f0ONZJ7v"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milvus-io/bootcamp/blob/master/tutorials/quickstart/contextual_retrieval_with_milvus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>   <a href=\"https://github.com/milvus-io/bootcamp/blob/master/tutorials/quickstart/contextual_retrieval_with_milvus.ipynb\" target=\"_blank\">\n",
        "    <img src=\"https://img.shields.io/badge/View%20on%20GitHub-555555?style=flat&logo=github&logoColor=white\" alt=\"GitHub Repository\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "KFworYXbZJ7w"
      },
      "source": [
        "# Contextual Retrieval with Milvus\n",
        "![image](https://raw.githubusercontent.com/milvus-io/bootcamp/refs/heads/master/images/contextual_retrieval_with_milvus.png)\n",
        "[Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval) is an advanced retrieval method proposed by Anthropic to address the issue of semantic isolation of chunks, which arises in current Retrieval-Augmented Generation (RAG) solutions. In the current practical RAG paradigm, documents are divided into several chunks, and a vector database is used to search for the query, retrieving the most relevant chunks. An LLM then responds to the query using these retrieved chunks. However, this chunking process can result in the loss of contextual information, making it difficult for the retriever to determine relevance.\n",
        "\n",
        "Contextual Retrieval improves traditional retrieval systems by adding relevant context to each document chunk before embedding or indexing, boosting accuracy and reducing retrieval errors. Combined with techniques like hybrid retrieval and reranking, it enhances Retrieval-Augmented Generation (RAG) systems, especially for large knowledge bases. Additionally, it offers a cost-effective solution when paired with prompt caching, significantly reducing latency and operational costs, with contextualized chunks costing approximately $1.02 per million document tokens. This makes it a scalable and efficient approach for handling large knowledge bases. Anthropic’s solution shows two insightful aspects:\n",
        "- `Document Enhancement`: Query rewriting is a crucial technique in modern information retrieval, often using auxiliary information to make the query more informative. Similarly, to achieve better performance in RAG, preprocessing documents with an LLM (e.g., cleaning the data source, complementing lost information, summarizing, etc.) before indexing can significantly improve the chances of retrieving relevant documents. In other words, this preprocessing step helps bring the documents closer to the queries in terms of relevance.\n",
        "- `Low-Cost Processing by Caching Long Context`: One common concern when using LLMs to process documents is the cost. The KVCache is a popular solution that allows reuse of intermediate results for the same preceding context. While most hosted LLM vendors make this feature transparent to user, Anthropic gives users control over the caching process. When a cache hit occurs, most computations can be saved (this is common when the long context remains the same, but the instruction for each query changes). For more details, click [here](https://www.anthropic.com/news/prompt-caching).\n",
        "\n",
        "In this notebook, we will demonstrate how to perform contextual retrieval using Milvus with an LLM, combining dense-sparse hybrid retrieval and a reranker to create a progressively more powerful retrieval system. The data and experimental setup are based on the [contextual retrieval](https://github.com/anthropics/anthropic-cookbook/blob/main/skills/contextual-embeddings/guide.ipynb).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86GgDvJbZJ7x"
      },
      "source": [
        "## Preparation\n",
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpyUBntCZJ7x",
        "outputId": "343feb92-1756-4963-8715-39c67a564070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymilvus[model] in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (2.6.5)\r\n",
            "Requirement already satisfied: setuptools>69 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pymilvus[model]) (80.9.0)\r\n",
            "Requirement already satisfied: grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pymilvus[model]) (1.76.0)\r\n",
            "Requirement already satisfied: orjson>=3.10.15 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pymilvus[model]) (3.11.5)\r\n",
            "Requirement already satisfied: protobuf>=5.27.2 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pymilvus[model]) (6.33.2)\r\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pymilvus[model]) (1.2.1)\r\n",
            "Requirement already satisfied: pandas>=1.2.4 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pymilvus[model]) (2.3.3)\r\n",
            "Requirement already satisfied: pymilvus.model>=0.3.0 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pymilvus[model]) (0.3.2)\r\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2->pymilvus[model]) (4.15.0)\r\n",
            "Requirement already satisfied: numpy>=1.26.0 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus[model]) (2.4.0)\r\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus[model]) (2.9.0.post0)\r\n",
            "Requirement already satisfied: pytz>=2020.1 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus[model]) (2025.2)\r\n",
            "Requirement already satisfied: tzdata>=2022.7 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus[model]) (2025.3)\r\n",
            "Requirement already satisfied: transformers>=4.36.0 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pymilvus.model>=0.3.0->pymilvus[model]) (4.57.3)\r\n",
            "Requirement already satisfied: onnxruntime in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pymilvus.model>=0.3.0->pymilvus[model]) (1.23.2)\r\n",
            "Requirement already satisfied: scipy>=1.10.0 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pymilvus.model>=0.3.0->pymilvus[model]) (1.16.3)\r\n",
            "Requirement already satisfied: six>=1.5 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus[model]) (1.17.0)\r\n",
            "Requirement already satisfied: filelock in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (3.20.1)\r\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (0.36.0)\r\n",
            "Requirement already satisfied: packaging>=20.0 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (25.0)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (6.0.3)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (2025.11.3)\r\n",
            "Requirement already satisfied: requests in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (2.32.5)\r\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (0.22.1)\r\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (0.7.0)\r\n",
            "Requirement already satisfied: tqdm>=4.27 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (2025.12.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (1.2.0)\n",
            "Requirement already satisfied: coloredlogs in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from onnxruntime->pymilvus.model>=0.3.0->pymilvus[model]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from onnxruntime->pymilvus.model>=0.3.0->pymilvus[model]) (25.12.19)\n",
            "Requirement already satisfied: sympy in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from onnxruntime->pymilvus.model>=0.3.0->pymilvus[model]) (1.14.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from coloredlogs->onnxruntime->pymilvus.model>=0.3.0->pymilvus[model]) (10.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from requests->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from requests->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from requests->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (2.6.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from requests->transformers>=4.36.0->pymilvus.model>=0.3.0->pymilvus[model]) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from sympy->onnxruntime->pymilvus.model>=0.3.0->pymilvus[model]) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (4.67.1)\n",
            "Requirement already satisfied: anthropic in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (0.71.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from anthropic) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from anthropic) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from anthropic) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from anthropic) (2.12.5)\n",
            "Requirement already satisfied: sniffio in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from anthropic) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from anyio<5,>=3.5.0->anthropic) (3.11)\n",
            "Requirement already satisfied: certifi in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from httpx<1,>=0.25.0->anthropic) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /workspace/.cache/uv/builds-v0/.tmp240B8K/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"pymilvus[model]\"\n",
        "!pip install tqdm\n",
        "!pip install anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "WycSuZuQZJ7y"
      },
      "source": [
        "> If you are using Google Colab, to enable dependencies just installed, you may need to **restart the runtime** (click on the \"Runtime\" menu at the top of the screen, and select \"Restart session\" from the dropdown menu)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUXFjRPLZJ7y"
      },
      "source": [
        "You will need API keys from Cohere, Voyage, and Anthropic to run the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-zIDliAZJ7y"
      },
      "source": [
        "## Download Data\n",
        "The following command will download the example data used in original Anthropic [demo](https://github.com/anthropics/anthropic-cookbook/blob/main/skills/contextual-embeddings/guide.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y0zo-0QZJ7z",
        "outputId": "637629ef-f64c-4d4d-9a13-4bd0bbd57001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-24 18:44:51--  https://raw.githubusercontent.com/anthropics/anthropic-cookbook/refs/heads/main/skills/contextual-embeddings/data/codebase_chunks.json\r\n",
            "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\r\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-12-24 18:44:51 ERROR 404: Not Found.\n",
            "\n",
            "--2025-12-24 18:44:51--  https://raw.githubusercontent.com/anthropics/anthropic-cookbook/refs/heads/main/skills/contextual-embeddings/data/evaluation_set.jsonl\n",
            "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-12-24 18:44:51 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/anthropics/anthropic-cookbook/refs/heads/main/skills/contextual-embeddings/data/codebase_chunks.json\n",
        "!wget https://raw.githubusercontent.com/anthropics/anthropic-cookbook/refs/heads/main/skills/contextual-embeddings/data/evaluation_set.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvZpeTJhZJ7z"
      },
      "source": [
        "## Define Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzkOXUzSZJ7z"
      },
      "source": [
        "This class is designed to be flexible, allowing you to choose between different retrieval modes based on your needs. By specifying options in the initialization method, you can determine whether to use contextual retrieval, hybrid search (combining dense and sparse retrieval methods), or a reranker for enhanced results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "w4UYjRSwZJ7z"
      },
      "outputs": [],
      "source": [
        "from pymilvus.model.dense import VoyageEmbeddingFunction\n",
        "from pymilvus.model.hybrid import BGEM3EmbeddingFunction\n",
        "from pymilvus.model.reranker import CohereRerankFunction\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "from typing import Callable\n",
        "from pymilvus import (\n",
        "    MilvusClient,\n",
        "    DataType,\n",
        "    AnnSearchRequest,\n",
        "    RRFRanker,\n",
        ")\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import anthropic\n",
        "import openai\n",
        "\n",
        "class MilvusContextualRetriever:\n",
        "    def __init__(\n",
        "        self,\n",
        "        uri=\"milvus.db\",\n",
        "        collection_name=\"contexual_bgem3\",\n",
        "        dense_embedding_function=None,\n",
        "        use_sparse=False,\n",
        "        sparse_embedding_function=None,\n",
        "        use_contextualize_embedding=False,\n",
        "        anthropic_client=None,\n",
        "        openai_client=None,\n",
        "        use_reranker=False,\n",
        "        rerank_function=None,\n",
        "    ):\n",
        "        self.collection_name = collection_name\n",
        "\n",
        "        # For Milvus-lite, uri is a local path like \"./milvus.db\"\n",
        "        # For Milvus standalone service, uri is like \"http://localhost:19530\"\n",
        "        # For Zilliz Clond, please set `uri` and `token`, which correspond to the [Public Endpoint and API key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#cluster-details) in Zilliz Cloud.\n",
        "        self.client = MilvusClient(uri)\n",
        "\n",
        "        self.embedding_function = dense_embedding_function\n",
        "\n",
        "        self.use_sparse = use_sparse\n",
        "        self.sparse_embedding_function = None\n",
        "\n",
        "        self.use_contextualize_embedding = use_contextualize_embedding\n",
        "        self.anthropic_client = anthropic_client\n",
        "        self.openai_client = openai.OpenAI\n",
        "\n",
        "        self.use_reranker = use_reranker\n",
        "        self.rerank_function = rerank_function\n",
        "\n",
        "        if use_sparse is True and sparse_embedding_function:\n",
        "            self.sparse_embedding_function = sparse_embedding_function\n",
        "        elif sparse_embedding_function is False:\n",
        "            raise ValueError(\n",
        "                \"Sparse embedding function cannot be None if use_sparse is False\"\n",
        "            )\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def build_collection(self):\n",
        "        schema = self.client.create_schema(\n",
        "            auto_id=True,\n",
        "            enable_dynamic_field=True,\n",
        "        )\n",
        "        schema.add_field(field_name=\"pk\", datatype=DataType.INT64, is_primary=True)\n",
        "        schema.add_field(\n",
        "            field_name=\"dense_vector\",\n",
        "            datatype=DataType.FLOAT_VECTOR,\n",
        "            dim=self.embedding_function.dim,\n",
        "        )\n",
        "        if self.use_sparse is True:\n",
        "            schema.add_field(\n",
        "                field_name=\"sparse_vector\", datatype=DataType.SPARSE_FLOAT_VECTOR\n",
        "            )\n",
        "\n",
        "        index_params = self.client.prepare_index_params()\n",
        "        index_params.add_index(\n",
        "            field_name=\"dense_vector\", index_type=\"FLAT\", metric_type=\"IP\"\n",
        "        )\n",
        "        if self.use_sparse is True:\n",
        "            index_params.add_index(\n",
        "                field_name=\"sparse_vector\",\n",
        "                index_type=\"SPARSE_INVERTED_INDEX\",\n",
        "                metric_type=\"IP\",\n",
        "            )\n",
        "\n",
        "        self.client.create_collection(\n",
        "            collection_name=self.collection_name,\n",
        "            schema=schema,\n",
        "            index_params=index_params,\n",
        "            enable_dynamic_field=True,\n",
        "        )\n",
        "\n",
        "    def insert_data(self, chunk, metadata):\n",
        "        dense_vec = self.embedding_function([chunk])[0]\n",
        "        if self.use_sparse is True:\n",
        "            sparse_result = self.sparse_embedding_function.encode_documents([chunk])\n",
        "            if type(sparse_result) == dict:\n",
        "                sparse_vec = sparse_result[\"sparse\"][[0]]\n",
        "            else:\n",
        "                sparse_vec = sparse_result[[0]]\n",
        "            self.client.insert(\n",
        "                collection_name=self.collection_name,\n",
        "                data={\n",
        "                    \"dense_vector\": dense_vec,\n",
        "                    \"sparse_vector\": sparse_vec,\n",
        "                    **metadata,\n",
        "                },\n",
        "            )\n",
        "        else:\n",
        "            self.client.insert(\n",
        "                collection_name=self.collection_name,\n",
        "                data={\"dense_vector\": dense_vec, **metadata},\n",
        "            )\n",
        "\n",
        "    def insert_contextualized_data(self, doc, chunk, metadata):\n",
        "        contextualized_text, usage = self.situate_context(doc, chunk)\n",
        "        metadata[\"context\"] = contextualized_text\n",
        "        text_to_embed = f\"{chunk}\\n\\n{contextualized_text}\"\n",
        "        dense_vec = self.embedding_function([text_to_embed])[0]\n",
        "        if self.use_sparse is True:\n",
        "            sparse_vec = self.sparse_embedding_function.encode_documents(\n",
        "                [text_to_embed]\n",
        "            )[\"sparse\"][[0]]\n",
        "            self.client.insert(\n",
        "                collection_name=self.collection_name,\n",
        "                data={\n",
        "                    \"dense_vector\": dense_vec,\n",
        "                    \"sparse_vector\": sparse_vec,\n",
        "                    **metadata,\n",
        "                },\n",
        "            )\n",
        "        else:\n",
        "            self.client.insert(\n",
        "                collection_name=self.collection_name,\n",
        "                data={\"dense_vector\": dense_vec, **metadata},\n",
        "            )\n",
        "\n",
        "    def situate_context(self, doc: str, chunk: str):\n",
        "        DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
        "        <document>\n",
        "        {doc_content}\n",
        "        </document>\n",
        "        \"\"\"\n",
        "\n",
        "        CHUNK_CONTEXT_PROMPT = \"\"\"\n",
        "        Here is the chunk we want to situate within the whole document\n",
        "        <chunk>\n",
        "        {chunk_content}\n",
        "        </chunk>\n",
        "\n",
        "        Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
        "        Answer only with the succinct context and nothing else.\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.openai_client.request(\n",
        "            model=\"chat\",\n",
        "            base_url=\"http://localhost:8002/v1\",\n",
        "            api_key=\"your-openai-api-key\",\n",
        "            max_tokens=1000,\n",
        "            temperature=0.0,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc),\n",
        "                            \"cache_control\": {\n",
        "                                \"type\": \"ephemeral\"\n",
        "                            },  # we will make use of prompt caching for the full documents\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n",
        "                        },\n",
        "                    ],\n",
        "                },\n",
        "            ],\n",
        "            extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"},\n",
        "        )\n",
        "        return response.content[0].text, response.usage\n",
        "\n",
        "    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
        "        dense_vec = self.embedding_function([query])[0]\n",
        "        if self.use_sparse is True:\n",
        "            sparse_vec = self.sparse_embedding_function.encode_queries([query])[\n",
        "                \"sparse\"\n",
        "            ][[0]]\n",
        "\n",
        "        req_list = []\n",
        "        if self.use_reranker:\n",
        "            k = k * 10\n",
        "        if self.use_sparse is True:\n",
        "            req_list = []\n",
        "            dense_search_param = {\n",
        "                \"data\": [dense_vec],\n",
        "                \"anns_field\": \"dense_vector\",\n",
        "                \"param\": {\"metric_type\": \"IP\"},\n",
        "                \"limit\": k * 2,\n",
        "            }\n",
        "            dense_req = AnnSearchRequest(**dense_search_param)\n",
        "            req_list.append(dense_req)\n",
        "\n",
        "            sparse_search_param = {\n",
        "                \"data\": [sparse_vec],\n",
        "                \"anns_field\": \"sparse_vector\",\n",
        "                \"param\": {\"metric_type\": \"IP\"},\n",
        "                \"limit\": k * 2,\n",
        "            }\n",
        "            sparse_req = AnnSearchRequest(**sparse_search_param)\n",
        "\n",
        "            req_list.append(sparse_req)\n",
        "\n",
        "            docs = self.client.hybrid_search(\n",
        "                self.collection_name,\n",
        "                req_list,\n",
        "                RRFRanker(),\n",
        "                k,\n",
        "                output_fields=[\n",
        "                    \"content\",\n",
        "                    \"original_uuid\",\n",
        "                    \"doc_id\",\n",
        "                    \"chunk_id\",\n",
        "                    \"original_index\",\n",
        "                    \"context\",\n",
        "                ],\n",
        "            )\n",
        "        else:\n",
        "            docs = self.client.search(\n",
        "                self.collection_name,\n",
        "                data=[dense_vec],\n",
        "                anns_field=\"dense_vector\",\n",
        "                limit=k,\n",
        "                output_fields=[\n",
        "                    \"content\",\n",
        "                    \"original_uuid\",\n",
        "                    \"doc_id\",\n",
        "                    \"chunk_id\",\n",
        "                    \"original_index\",\n",
        "                    \"context\",\n",
        "                ],\n",
        "            )\n",
        "        if self.use_reranker and self.use_contextualize_embedding:\n",
        "            reranked_texts = []\n",
        "            reranked_docs = []\n",
        "            for i in range(k):\n",
        "                if self.use_contextualize_embedding:\n",
        "                    reranked_texts.append(\n",
        "                        f\"{docs[0][i]['entity']['content']}\\n\\n{docs[0][i]['entity']['context']}\"\n",
        "                    )\n",
        "                else:\n",
        "                    reranked_texts.append(f\"{docs[0][i]['entity']['content']}\")\n",
        "            results = self.rerank_function(query, reranked_texts)\n",
        "            for result in results:\n",
        "                reranked_docs.append(docs[0][result.index])\n",
        "            docs[0] = reranked_docs\n",
        "        return docs\n",
        "\n",
        "\n",
        "def evaluate_retrieval(\n",
        "    queries: List[Dict[str, Any]], retrieval_function: Callable, db, k: int = 20\n",
        ") -> Dict[str, float]:\n",
        "    total_score = 0\n",
        "    total_queries = len(queries)\n",
        "    for query_item in tqdm(queries, desc=\"Evaluating retrieval\"):\n",
        "        query = query_item[\"query\"]\n",
        "        golden_chunk_uuids = query_item[\"golden_chunk_uuids\"]\n",
        "\n",
        "        # Find all golden chunk contents\n",
        "        golden_contents = []\n",
        "        for doc_uuid, chunk_index in golden_chunk_uuids:\n",
        "            golden_doc = next(\n",
        "                (\n",
        "                    doc\n",
        "                    for doc in query_item[\"golden_documents\"]\n",
        "                    if doc[\"uuid\"] == doc_uuid\n",
        "                ),\n",
        "                None,\n",
        "            )\n",
        "            if not golden_doc:\n",
        "                print(f\"Warning: Golden document not found for UUID {doc_uuid}\")\n",
        "                continue\n",
        "\n",
        "            golden_chunk = next(\n",
        "                (\n",
        "                    chunk\n",
        "                    for chunk in golden_doc[\"chunks\"]\n",
        "                    if chunk[\"index\"] == chunk_index\n",
        "                ),\n",
        "                None,\n",
        "            )\n",
        "            if not golden_chunk:\n",
        "                print(\n",
        "                    f\"Warning: Golden chunk not found for index {chunk_index} in document {doc_uuid}\"\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            golden_contents.append(golden_chunk[\"content\"].strip())\n",
        "\n",
        "        if not golden_contents:\n",
        "            print(f\"Warning: No golden contents found for query: {query}\")\n",
        "            continue\n",
        "\n",
        "        retrieved_docs = retrieval_function(query, db, k=k)\n",
        "\n",
        "        # Count how many golden chunks are in the top k retrieved documents\n",
        "        chunks_found = 0\n",
        "        for golden_content in golden_contents:\n",
        "            for doc in retrieved_docs[0][:k]:\n",
        "                retrieved_content = doc[\"entity\"][\"content\"].strip()\n",
        "                if retrieved_content == golden_content:\n",
        "                    chunks_found += 1\n",
        "                    break\n",
        "\n",
        "        query_score = chunks_found / len(golden_contents)\n",
        "        total_score += query_score\n",
        "\n",
        "    average_score = total_score / total_queries\n",
        "    pass_at_n = average_score * 100\n",
        "    return {\n",
        "        \"pass_at_n\": pass_at_n,\n",
        "        \"average_score\": average_score,\n",
        "        \"total_queries\": total_queries,\n",
        "    }\n",
        "\n",
        "\n",
        "def retrieve_base(query: str, db, k: int = 20) -> List[Dict[str, Any]]:\n",
        "    return db.search(query, k=k)\n",
        "\n",
        "\n",
        "def load_jsonl(file_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Load JSONL file and return a list of dictionaries.\"\"\"\n",
        "    with open(file_path, \"r\") as file:\n",
        "        return [json.loads(line) for line in file]\n",
        "\n",
        "\n",
        "def evaluate_db(db, original_jsonl_path: str, k):\n",
        "    # Load the original JSONL data for queries and ground truth\n",
        "    original_data = load_jsonl(original_jsonl_path)\n",
        "\n",
        "    # Evaluate retrieval\n",
        "    results = evaluate_retrieval(original_data, retrieve_base, db, k)\n",
        "    print(f\"Pass@{k}: {results['pass_at_n']:.2f}%\")\n",
        "    print(f\"Total Score: {results['average_score']}\")\n",
        "    print(f\"Total queries: {results['total_queries']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwftjImoZJ71"
      },
      "source": [
        "Now you need to initialize these models for the following experiments. You can easily switch to other models using the PyMilvus model library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "1b2b4f965e814e7e81593ede09243b39",
            "b0d318ec302f4b1581b885d10e4305c8",
            "16f44fa373904893aae9db149bbb40c1",
            "df077cb4ab1a4071bfc9e66a33a4beee",
            "37048f0efbdb4ed2bd1ee1442990643d",
            "9d0641e96c024177b218f3f53d748ed0",
            "ab127c4c65e64f75ac248be7df267a01",
            "f916b94848344c1e996276acca9cbc63",
            "5d870f1514354cf69f8a902e685d9255",
            "6acd0179b25c48b489676e00da8ef3ac",
            "a303345079414bd9ad151c0cb5d6aede"
          ]
        },
        "id": "cPpf6NmSZJ72",
        "outputId": "047e1ca7-4eb7-4984-cd75-61c71f1fcbc1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b2b4f965e814e7e81593ede09243b39"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# dense_ef = VoyageEmbeddingFunction(api_key=\"your-voyage-api-key\", model_name=\"voyage-2\")\n",
        "from pymilvus import model\n",
        "import openai\n",
        "\n",
        "dense_ef = model.dense.OpenAIEmbeddingFunction(\n",
        "      model_name='embed', # Specify the model name\n",
        "      api_key='YOUR_API_KEY', # Provide your OpenAI API key\n",
        "      dimensions=1024, # Set the embedding dimensionality\n",
        "      base_url = 'http://localhost:8005/v1' # Set the base URL for the OpenAI API\n",
        ")\n",
        "\n",
        "sparse_ef = BGEM3EmbeddingFunction()\n",
        "cohere_rf = CohereRerankFunction(api_key=\"your-cohere-api-key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "9_alPLm-ZJ72"
      },
      "outputs": [],
      "source": [
        "path = \"codebase_chunks.json\"\n",
        "with open(path, \"r\") as f:\n",
        "    dataset = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MHeCnvJZJ72"
      },
      "source": [
        "## Experiment I: Standard Retrieval\n",
        "Standard retrieval uses only dense embeddings to retrieve related documents. In this experiment, we will use Pass@5 to reproduce the results from the original repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnZbS-xEZJ73",
        "outputId": "73e4f996-cdbd-4262-e314-1e7d419df2f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "standard_retriever = MilvusContextualRetriever(\n",
        "    uri=\"standard.db\", collection_name=\"standard\", dense_embedding_function=dense_ef\n",
        ")\n",
        "\n",
        "# Drop the collection if it already exists to avoid the 'duplicate collection' error\n",
        "if standard_retriever.client.has_collection(collection_name=\"standard\"):\n",
        "    standard_retriever.client.drop_collection(collection_name=\"standard\")\n",
        "\n",
        "standard_retriever.build_collection()\n",
        "for doc in dataset:\n",
        "    doc_content = doc[\"content\"]\n",
        "    for chunk in doc[\"chunks\"]:\n",
        "        metadata = {\n",
        "            \"doc_id\": doc[\"doc_id\"],\n",
        "            \"original_uuid\": doc[\"original_uuid\"],\n",
        "            \"chunk_id\": chunk[\"chunk_id\"],\n",
        "            \"original_index\": chunk[\"original_index\"],\n",
        "            \"content\": chunk[\"content\"],\n",
        "        }\n",
        "        chunk_content = chunk[\"content\"]\n",
        "        standard_retriever.insert_data(chunk_content, metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YymKVbUqZJ73"
      },
      "outputs": [],
      "source": [
        "evaluate_db(standard_retriever, \"evaluation_set.jsonl\", 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4ppUg2pZJ73"
      },
      "source": [
        "## Experiment II: Hybrid Retrieval\n",
        "Now that we've obtained promising results with the Voyage embedding, we will move on to performing hybrid retrieval using the BGE-M3 model which generates powerful sparse embeddings. The results from dense retrieval and sparse retrieval will be combined using the Reciprocal Rank Fusion (RRF) method to produce a hybrid result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh9pzpdeZJ73"
      },
      "outputs": [],
      "source": [
        "hybrid_retriever = MilvusContextualRetriever(\n",
        "    uri=\"hybrid.db\",\n",
        "    collection_name=\"hybrid\",\n",
        "    dense_embedding_function=dense_ef,\n",
        "    use_sparse=True,\n",
        "    sparse_embedding_function=sparse_ef,\n",
        ")\n",
        "\n",
        "hybrid_retriever.build_collection()\n",
        "for doc in dataset:\n",
        "    doc_content = doc[\"content\"]\n",
        "    for chunk in doc[\"chunks\"]:\n",
        "        metadata = {\n",
        "            \"doc_id\": doc[\"doc_id\"],\n",
        "            \"original_uuid\": doc[\"original_uuid\"],\n",
        "            \"chunk_id\": chunk[\"chunk_id\"],\n",
        "            \"original_index\": chunk[\"original_index\"],\n",
        "            \"content\": chunk[\"content\"],\n",
        "        }\n",
        "        chunk_content = chunk[\"content\"]\n",
        "        hybrid_retriever.insert_data(chunk_content, metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wosLUi2nZJ73"
      },
      "outputs": [],
      "source": [
        "evaluate_db(hybrid_retriever, \"evaluation_set.jsonl\", 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwDf-cbjZJ74"
      },
      "source": [
        "## Experiment III: Contextual Retrieval\n",
        "Hybrid retrieval shows an improvement, but the results can be further enhanced by applying a contextual retrieval method. To achieve this, we will use Anthropic's language model to prepend the context from whole document for each chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxaNIuKrZJ74"
      },
      "outputs": [],
      "source": [
        "anthropic_client = anthropic.Anthropic(\n",
        "    api_key=\"your-anthropic-api-key\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X09E0Q9HZJ74"
      },
      "outputs": [],
      "source": [
        "contextual_retriever = MilvusContextualRetriever(\n",
        "    uri=\"contextual.db\",\n",
        "    collection_name=\"contextual\",\n",
        "    dense_embedding_function=dense_ef,\n",
        "    use_sparse=True,\n",
        "    sparse_embedding_function=sparse_ef,\n",
        "    use_contextualize_embedding=True,\n",
        "    anthropic_client=anthropic_client,\n",
        ")\n",
        "\n",
        "contextual_retriever.build_collection()\n",
        "for doc in dataset:\n",
        "    doc_content = doc[\"content\"]\n",
        "    for chunk in doc[\"chunks\"]:\n",
        "        metadata = {\n",
        "            \"doc_id\": doc[\"doc_id\"],\n",
        "            \"original_uuid\": doc[\"original_uuid\"],\n",
        "            \"chunk_id\": chunk[\"chunk_id\"],\n",
        "            \"original_index\": chunk[\"original_index\"],\n",
        "            \"content\": chunk[\"content\"],\n",
        "        }\n",
        "        chunk_content = chunk[\"content\"]\n",
        "        contextual_retriever.insert_contextualized_data(\n",
        "            doc_content, chunk_content, metadata\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7usuZV4oZJ74"
      },
      "outputs": [],
      "source": [
        "evaluate_db(contextual_retriever, \"evaluation_set.jsonl\", 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKFF7SqbZJ74"
      },
      "source": [
        "## Experiment IV: Contextual Retrieval with Reranker\n",
        "The results can be further improved by adding a Cohere reranker. Without initializing a new retriever with reranker separately, we can simply configure the existing retriever to use the reranker for enhanced performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlQ_YxbqZJ74"
      },
      "outputs": [],
      "source": [
        "contextual_retriever.use_reranker = True\n",
        "contextual_retriever.rerank_function = cohere_rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH0s2UK1ZJ74"
      },
      "outputs": [],
      "source": [
        "evaluate_db(contextual_retriever, \"evaluation_set.jsonl\", 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "its4uH0BZJ74"
      },
      "source": [
        "We have demonstrated several methods to improve retrieval performance. With more ad-hoc design tailored to the scenario, contextual retrieval shows significant potential to preprocess documents at a low cost, leading to a better RAG system."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b2b4f965e814e7e81593ede09243b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0d318ec302f4b1581b885d10e4305c8",
              "IPY_MODEL_16f44fa373904893aae9db149bbb40c1",
              "IPY_MODEL_df077cb4ab1a4071bfc9e66a33a4beee"
            ],
            "layout": "IPY_MODEL_37048f0efbdb4ed2bd1ee1442990643d",
            "tabbable": null,
            "tooltip": null
          }
        },
        "b0d318ec302f4b1581b885d10e4305c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_9d0641e96c024177b218f3f53d748ed0",
            "placeholder": "​",
            "style": "IPY_MODEL_ab127c4c65e64f75ac248be7df267a01",
            "tabbable": null,
            "tooltip": null,
            "value": "Fetching 30 files: 100%"
          }
        },
        "16f44fa373904893aae9db149bbb40c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_f916b94848344c1e996276acca9cbc63",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d870f1514354cf69f8a902e685d9255",
            "tabbable": null,
            "tooltip": null,
            "value": 30
          }
        },
        "df077cb4ab1a4071bfc9e66a33a4beee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_6acd0179b25c48b489676e00da8ef3ac",
            "placeholder": "​",
            "style": "IPY_MODEL_a303345079414bd9ad151c0cb5d6aede",
            "tabbable": null,
            "tooltip": null,
            "value": " 30/30 [00:00&lt;00:00, 6108.21it/s]"
          }
        },
        "37048f0efbdb4ed2bd1ee1442990643d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d0641e96c024177b218f3f53d748ed0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab127c4c65e64f75ac248be7df267a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "f916b94848344c1e996276acca9cbc63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d870f1514354cf69f8a902e685d9255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6acd0179b25c48b489676e00da8ef3ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a303345079414bd9ad151c0cb5d6aede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}