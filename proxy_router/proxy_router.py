# proxy_router.py ─ unified GPU router / gatekeeper
import threading, json, httpx, os
from dotenv import load_dotenv
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, StreamingResponse
from .supervisor import (
    record_activity, run_supervisor, is_model_awake,
    wake_model
)
from typing import TypedDict, Annotated, List, Dict
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
import operator
from langgraph.prebuilt import ToolNode
from langgraph.graph import StateGraph, END
from proxy_router.tools import search_file_content
from langchain.memory import ConversationBufferWindowMemory

load_dotenv()

# GPU-bound services and their local ports
MODEL_PORTS = {
    "llama.cpp-embedding": 8001,
    "vllm-agent":          8002,
    "faster-whisper":      8003,
}

# ───────────────────────────────────────────────────────────────
# Path  →  target-service map
#   (exact match is enough because we keep the endpoints short)
# ───────────────────────────────────────────────────────────────
MODEL_BY_PATH = {
    "/v1/embeddings":          "llama.cpp-embedding",
    "/v1/chat/completions":    "vllm-agent",

    # Faster-Whisper endpoints
    "/transcribe":             "faster-whisper",
    "/transcribe_url":         "faster-whisper",
    # vllm-agent wake endpoint
    "/wake_up":                "vllm-agent",
}

TIMEOUT = float(os.getenv("PROXY_TIMEOUT", 1800))

# Global cache for tool definitions
TOOL_DEFINITIONS_CACHE = {}

def fetch_and_cache_tool_definitions():
    """
    Fetches the OpenAPI schema from the vllm-agent service and caches it.
    """
    global TOOL_DEFINITIONS_CACHE
    try:
        # Assuming vllm-agent is running on port 8002 and exposes OpenAPI schema at /openapi.json
        url = f"{os.getenv('VLLM_URL')}/openapi.json"
        with httpx.Client() as client:
            response = client.get(url)
            response.raise_for_status()
            openapi_schema = response.json()
            # Extract tool definitions in a format compatible with vLLM
            # Assuming the vLLM expects a list of tool objects, each with 'type' and 'function'
            # and the function has 'name' and 'description' and 'parameters'
            tools_list = []
            if "paths" in openapi_schema:
                for path, methods in openapi_schema["paths"].items():
                    for method, details in methods.items():
                        if "x-openai-is_tool" in details and details["x-openai-is_tool"]:
                            tool_name = details["operationId"]
                            tool_description = details.get("description", "")
                            parameters = details.get("parameters", [])
                            
                            # Convert OpenAPI parameters to JSON Schema format
                            json_schema_parameters = {"type": "object", "properties": {}, "required": []}
                            for param in parameters:
                                param_name = param["name"]
                                json_schema_parameters["properties"][param_name] = {"type": param["schema"]["type"]}
                                if param.get("required", False):
                                    json_schema_parameters["required"].append(param_name)
                            
                            tools_list.append({
                                "type": "function",
                                "function": {
                                    "name": tool_name,
                                    "description": tool_description,
                                    "parameters": json_schema_parameters
                                }
                            })
            print(f"Formatted tools_list: {tools_list}")
            TOOL_DEFINITIONS_CACHE = tools_list
            print(f"TOOL_DEFINITIONS_CACHE after formatting: {TOOL_DEFINITIONS_CACHE}")
            print("Successfully fetched and cached tool definitions.")
    except httpx.RequestError as e:
        print(f"Error fetching tool definitions: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

tools = [search_file_content]
tool_node = ToolNode(tools)

# In-memory store for conversation history
conversation_store: Dict[str, ConversationBufferWindowMemory] = {}

class AgentState(TypedDict):
    # A unique identifier for the conversation
    conversation_id: str
    # The user's input
    input: str
    # The conversation history
    chat_history: list[BaseMessage]
    # The list of tool calls generated by the LLM
    agent_outcome: List[dict]
    # The results of the tool executions
    intermediate_steps: Annotated[list[ToolMessage], operator.add]

def construct_message_list(state: AgentState) -> list:
    """
    Constructs the list of messages to be sent to the LLM.
    """
    messages = []
    # Add the chat history, converting BaseMessage to dict format expected by vLLM
    for msg in state.get("chat_history", []):
        if isinstance(msg, HumanMessage):
            messages.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            messages.append({"role": "assistant", "content": msg.content})
        # Add other message types if necessary

    # Add the user's current input
    messages.append({"role": "user", "content": state["input"]})

    # Add the results of any intermediate tool executions
    if "intermediate_steps" in state:
        for tool_message in state["intermediate_steps"]:
            messages.append(tool_message)
    return messages


def call_model(state: AgentState) -> dict:
    """
    Invokes the vLLM with the current state and returns the LLM's response.
    """
    conversation_id = state["conversation_id"]
    if conversation_id not in conversation_store:
        conversation_store[conversation_id] = ConversationBufferWindowMemory(k=10, return_messages=True)

    memory = conversation_store[conversation_id]
    
    # Load history into state
    state["chat_history"] = memory.load_memory_variables({})['history']

    messages = construct_message_list(state)
    
    vllm_url = f"{os.getenv('VLLM_URL')}/v1/chat/completions"
    
    with httpx.Client() as client:
        print(f"Sending messages to vLLM: {messages}")
        print(f"Sending tools to vLLM: {TOOL_DEFINITIONS_CACHE}")
        response = client.post(
            vllm_url,
            json={"messages": messages, "tools": TOOL_DEFINITIONS_CACHE},
            headers={"Content-Type": "application/json"}
        )
        response.raise_for_status()
        llm_response = response.json()

    # Save new interaction to memory
    memory.save_context({"input": state["input"]}, {"output": llm_response["choices"][0]["message"]["content"]})
    
    return {"agent_outcome": [llm_response]}

def should_continue(state: AgentState) -> str:
    """
    Determines whether to continue with tool calls or stop.
    """
    if "agent_outcome" not in state or not state["agent_outcome"]:
        return "end"
    if state["agent_outcome"][-1].get("tool_calls"):
        return "continue"
    return "end"


def call_tool_node(state: AgentState) -> dict:
    """
    Executes the tool node and formats the output.
    """
    result = tool_node.invoke(state)
    return {"intermediate_steps": result.get("messages", [])}

def create_graph():
    """
    Creates the agent graph.
    """
    workflow = StateGraph(AgentState)

    workflow.add_node("agent", call_model)
    workflow.add_node("action", call_tool_node)

    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "continue": "action",
            "end": END,
        },
    )

    workflow.add_edge("action", "agent")

    workflow.set_entry_point("agent")

    return workflow.compile()

app = FastAPI()

# ---------------------------------------------------------------------------
# middleware: intercept only the known API paths, forward everything else
# ---------------------------------------------------------------------------
@app.middleware("http")
async def route_request(request: Request, call_next):
    if request.url.path not in MODEL_BY_PATH:
        # static files, docs, or unknown path → let FastAPI handle
        return await call_next(request)

    model_key = MODEL_BY_PATH[request.url.path]
    port      = MODEL_PORTS[model_key]

    # If this is a transcribe request, wake up vllm-agent first
    if model_key == "faster-whisper" and request.url.path in ("/transcribe", "/transcribe_url"):
        from supervisor import wake_model
        wake_model("vllm-agent")

    # Spin container up or wake model if needed
    if not is_model_awake(model_key):
        wake_model(model_key)
        return JSONResponse(
            status_code=202,
            content={"status": f"{model_key} waking – retry in 2 s"}
        )

    record_activity(model_key)
    target_url = f"http://localhost:{port}{request.url.path}"

    try:
        async with httpx.AsyncClient(timeout=TIMEOUT) as client:
            prox_req = client.build_request(
                method  = request.method,
                url     = target_url,
                headers = request.headers.raw,
                content = await request.body()
            )
            prox_resp = await client.send(prox_req, stream=True)
            response = StreamingResponse(
                prox_resp.aiter_bytes(),
                status_code = prox_resp.status_code,
                media_type  = prox_resp.headers.get("content-type"),
                headers     = {k: v for k, v in prox_resp.headers.items()
                               if k.lower().startswith("content-")}
            )
            # ...container stop logic removed...
            return response
    except httpx.ReadTimeout:
        return JSONResponse(status_code=504,
                            content={"error": f"{model_key} timed-out"})
    except Exception as exc:
        return JSONResponse(status_code=500,
                            content={"error": f"proxy error: {exc}"})

# Global variable to hold the compiled graph
fetch_and_cache_tool_definitions()
app_graph = create_graph() # Compile the graph once at startup

@app.post("/chat")
async def chat_endpoint(request: Request):
    body = await request.json()
    conversation_id = body.get("conversation_id", "default")  # Use a default if not provided
    user_input = body.get("input")
    messages = body.get("messages", [])

    if not user_input:
        return JSONResponse(status_code=400, content={"error": "Input cannot be empty"})

    # Create the initial state for the graph
    initial_state = AgentState(
        conversation_id=conversation_id,
        input=user_input,
        chat_history=messages,
        agent_outcome=[],
        intermediate_steps=[]
    )

    # Compile the graph if not already compiled
    global app_graph
    if app_graph is None:
        app_graph = create_graph()

    # Invoke the graph
    final_state = app_graph.invoke(initial_state)
    print(f"Final state after graph invocation: {final_state}")

    # Extract the final answer from the agent_outcome
    final_answer = ""
    if final_state and final_state["agent_outcome"]:
        # Assuming the last agent_outcome contains the final message from the LLM
        # This might need adjustment based on the actual structure of llm_response
        last_outcome = final_state["agent_outcome"][-1]
        if "choices" in last_outcome and last_outcome["choices"]:
            final_answer = last_outcome["choices"][0]["message"]["content"]
        elif "content" in last_outcome: # For cases where agent_outcome directly contains content
            final_answer = last_outcome["content"]

    return JSONResponse(content={"conversation_id": conversation_id, "response": final_answer})

# ---------------------------------------------------------------------------
# Launch router + background supervisor
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    threading.Thread(target=run_supervisor, daemon=True).start()
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=9000)
