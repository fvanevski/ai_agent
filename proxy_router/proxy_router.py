# proxy_router.py ─ unified GPU router / gatekeeper
import threading, json, httpx, os
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, StreamingResponse
from supervisor import (
    record_activity, run_supervisor, is_model_awake,
    wake_model
)
from typing import TypedDict, Annotated, List
from langchain_core.messages import BaseMessage
import operator

class AgentState(TypedDict):
    # The user's input
    input: str
    # The conversation history
    chat_history: list[BaseMessage]
    # The list of tool calls generated by the LLM
    agent_outcome: List[dict]
    # The results of the tool executions
    intermediate_steps: Annotated[list[tuple[str, str]], operator.add]

# Global cache for tool definitions
TOOL_DEFINITIONS_CACHE = {}

def fetch_and_cache_tool_definitions():
    """
    Fetches the OpenAPI schema from the vllm-agent service and caches it.
    """
    global TOOL_DEFINITIONS_CACHE
    try:
        # Assuming vllm-agent is running on port 8002 and exposes OpenAPI schema at /openapi.json
        url = "http://localhost:8002/openapi.json"
        with httpx.Client() as client:
            response = client.get(url)
            response.raise_for_status()
            openapi_schema = response.json()
            # In a real scenario, you would parse the openapi_schema to extract tool definitions
            # For now, we'll just cache the whole schema
            TOOL_DEFINITIONS_CACHE = openapi_schema
            print("Successfully fetched and cached tool definitions.")
    except httpx.RequestError as e:
        print(f"Error fetching tool definitions: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")


app = FastAPI()

# GPU-bound services and their local ports
MODEL_PORTS = {
    "llama.cpp-embedding": 8001,
    "vllm-agent":          8002,
    "faster-whisper":      8003,
}

# ───────────────────────────────────────────────────────────────
# Path  →  target-service map
#   (exact match is enough because we keep the endpoints short)
# ───────────────────────────────────────────────────────────────
MODEL_BY_PATH = {
    "/v1/embeddings":          "llama.cpp-embedding",
    "/v1/chat/completions":    "vllm-agent",

    # Faster-Whisper endpoints
    "/transcribe":             "faster-whisper",
    "/transcribe_url":         "faster-whisper",
    # vllm-agent wake endpoint
    "/wake_up":                "vllm-agent",
}

TIMEOUT = float(os.getenv("PROXY_TIMEOUT", 1800))

# ---------------------------------------------------------------------------
# middleware: intercept only the known API paths, forward everything else
# ---------------------------------------------------------------------------
@app.middleware("http")
async def route_request(request: Request, call_next):
    if request.url.path not in MODEL_BY_PATH:
        # static files, docs, or unknown path → let FastAPI handle
        return await call_next(request)

    model_key = MODEL_BY_PATH[request.url.path]
    port      = MODEL_PORTS[model_key]

    # If this is a transcribe request, wake up vllm-agent first
    if model_key == "faster-whisper" and request.url.path in ("/transcribe", "/transcribe_url"):
        from supervisor import wake_model
        wake_model("vllm-agent")

    # Spin container up or wake model if needed
    if not is_model_awake(model_key):
        wake_model(model_key)
        return JSONResponse(
            status_code=202,
            content={"status": f"{model_key} waking – retry in 2 s"}
        )

    record_activity(model_key)
    target_url = f"http://localhost:{port}{request.url.path}"

    try:
        async with httpx.AsyncClient(timeout=TIMEOUT) as client:
            prox_req = client.build_request(
                method  = request.method,
                url     = target_url,
                headers = request.headers.raw,
                content = await request.body()
            )
            prox_resp = await client.send(prox_req, stream=True)
            response = StreamingResponse(
                prox_resp.aiter_bytes(),
                status_code = prox_resp.status_code,
                media_type  = prox_resp.headers.get("content-type"),
                headers     = {k: v for k, v in prox_resp.headers.items()
                               if k.lower().startswith("content-")}
            )
            # ...container stop logic removed...
            return response
    except httpx.ReadTimeout:
        return JSONResponse(status_code=504,
                            content={"error": f"{model_key} timed-out"})
    except Exception as exc:
        return JSONResponse(status_code=500,
                            content={"error": f"proxy error: {exc}"})

def construct_message_list(state: AgentState) -> list:
    """
    Constructs the list of messages to be sent to the LLM.
    """
    # Start with a system prompt if you have one
    messages = []
    # Add the chat history
    messages.extend(state.get("chat_history", []))
    # Add the user's input
    messages.append({"role": "user", "content": state["input"]})
    # Add the results of any intermediate tool executions
    if "intermediate_steps" in state:
        for tool_name, tool_output in state["intermediate_steps"]:
            messages.append({"role": "tool", "tool_call_id": tool_name, "content": tool_output})
    return messages

# ---------------------------------------------------------------------------
# Launch router + background supervisor
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    fetch_and_cache_tool_definitions()
    threading.Thread(target=run_supervisor, daemon=True).start()
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=9000)
