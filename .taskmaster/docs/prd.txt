<context>
# Overview
This document provides a detailed technical plan for implementing the AI agent's orchestrator layer using LangGraph. The focus is on creating a robust, stateful, and multi-tool workflow that can intelligently process user requests by calling one or more tools via FastAPI endpoints and then cycling the results back to the vLLM for final processing.

# Core Features
- **Stateful Graph-Based Orchestration:** Use LangGraph to manage the agent's state, including conversation history, tool calls, and intermediate results.
- **Dynamic Tool Execution:** The orchestrator will parse tool call requests from the LLM and execute them against the appropriate FastAPI-MCP-exposed endpoints.
- **Multi-Tool & Parallel Workflows:** The agent will be capable of executing multiple tools in sequence or in parallel, based on the LLM's instructions, to resolve complex queries.
- **Iterative Reasoning Loop:** The orchestrator will feed the results of tool executions back to the LLM, allowing for a "Perception ⇒ Reflexion ⇒ Action" cycle until the user's request is fully addressed.
</context>
<PRD>
# Technical Architecture

## 1. Agent State (`AgentState`)
The core of the LangGraph implementation will be a typed dictionary representing the agent's state. This state will be passed between all nodes in the graph.

```python
from typing import TypedDict, Annotated, List
from langchain_core.messages import BaseMessage
import operator

class AgentState(TypedDict):
    # The user's input
    input: str
    # The conversation history
    chat_history: list[BaseMessage]
    # The list of tool calls generated by the LLM
    agent_outcome: List[dict]
    # The results of the tool executions
    intermediate_steps: Annotated[list[tuple[str, str]], operator.add]
```

## 2. LangGraph Orchestrator Graph
We will define a `StateMachine` graph with nodes representing the agent's core logic and edges defining the flow of control.

### Nodes:
- **`call_model`**: This node is responsible for interacting with the vLLM. It will take the current `AgentState` (user input, chat history, and previous tool results) and call the LLM. The LLM's response, which could be a direct answer or a `tool_call` request, will be parsed and used to update the `agent_outcome` in the state.
- **`call_tool`**: This node, likely a `ToolNode` from LangGraph, will process the `agent_outcome`. It will iterate through the list of tool calls, execute each one against the corresponding FastAPI endpoint, and append the results to the `intermediate_steps`. It will be designed to handle both single and multiple tool calls.
- **`should_continue`**: This is a conditional routing node. It will inspect the `agent_outcome` from the `call_model` node.
    - If `agent_outcome` contains tool calls, it will route the graph to the `call_tool` node.
    - If `agent_outcome` is a direct response (no tool calls), it will route the graph to the `END` state.

### Edges:
1.  **Entry Point**: The graph will start at the `call_model` node.
2.  **Conditional Routing**: An edge will connect `call_model` to `should_continue`.
3.  **Tool Execution Path**: An edge will connect `should_continue` to `call_tool` if tools need to be called.
4.  **Reasoning Loop**: An edge will connect `call_tool` back to `call_model`, feeding the tool results back to the LLM for the next step in the reasoning process.
5.  **Exit Path**: An edge will connect `should_continue` to `END` when the process is complete.

## 3. Tool Integration
- Tools will be defined on the FastAPI server and exposed via FastAPI-MCP.
- The orchestrator will fetch these tool definitions at startup.
- The `call_model` node will inject these tool definitions into the prompt sent to the vLLM, allowing the model to discover and choose the appropriate tools.

# Development Roadmap

## Phase 1: MVP - Single Tool Sequential Workflow
1.  **Define `AgentState`**: Implement the core state management dictionary.
2.  **Implement `call_model` Node**: Write the function to call the vLLM, including prompt construction with tool definitions.
3.  **Implement `call_tool` Node**: Write the function to execute a single tool call from the `agent_outcome`.
4.  **Build the Graph**: Assemble the nodes and edges in LangGraph for a sequential, single-tool workflow.
5.  **Integration**: Connect the LangGraph orchestrator to the vLLM server and a single tool endpoint on the FastAPI server.

## Phase 2: Multi-Tool & Parallel Execution
1.  **Enhance `call_tool` Node**: Modify the node to iterate through a list of tool calls in `agent_outcome` and execute them sequentially.
2.  **Investigate Parallel Execution**: Research and implement parallel tool execution for independent tool calls identified by the LLM. This may involve using Python's `asyncio` or `concurrent.futures` within the `call_tool` node.
3.  **Update LLM Prompting**: Refine the system prompt to encourage the LLM to identify and request parallelizable tasks.

## Phase 3: Robustness and Error Handling
1.  **Error Handling in `call_tool`**: Implement `try...except` blocks to gracefully handle failures in tool execution (e.g., API errors, invalid parameters).
2.  **Feedback Loop for Errors**: When a tool fails, format the error message and append it to `intermediate_steps`. The `call_model` node will then receive this error context, allowing the LLM to potentially correct its course or inform the user.
3.  **Stateful Conversation History**: Integrate `ConversationBufferWindowMemory` to manage `chat_history` in the `AgentState`, ensuring the agent has context for multi-turn dialogues.

# Logical Dependency Chain
1.  A functioning vLLM server with a tool-calling-capable model must be available.
2.  The FastAPI server with at least one tool endpoint exposed via FastAPI-MCP must be running.
3.  Phase 1 must be complete before starting Phase 2, as the multi-tool logic builds upon the single-tool foundation.
4.  Phase 3 can be developed in parallel with Phase 2, but robust error handling is critical before considering the orchestrator production-ready.

# Risks and Mitigations
- **Risk**: The chosen LLM may not be reliable at generating correct multi-tool call syntax or identifying parallel opportunities.
- **Mitigation**: We will start with models known for strong tool-calling abilities (e.g., Llama 3.1, specific fine-tunes). We will also employ rigorous prompt engineering and few-shot examples to guide the model's behavior.
- **Risk**: Complex dependencies between tools could make parallel execution difficult to manage.
- **Mitigation**: The initial implementation will focus on "embarrassingly parallel" tasks. For complex dependencies, we will rely on the LLM to generate a sequential plan, which the graph will execute step-by-step.
</PRD>