{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Define `AgentState` TypedDict for Graph State Management",
        "description": "Implement the core `AgentState` typed dictionary as specified in the PRD. This structure will hold the agent's state, including input, history, tool outcomes, and intermediate results, and will be passed between all nodes in the LangGraph.",
        "details": "Create a Python file (e.g., `state.py`) and define the `AgentState` class using `typing.TypedDict`. Ensure all specified fields are included with their correct types as per the PRD:\n```python\nfrom typing import TypedDict, Annotated, List\nfrom langchain_core.messages import BaseMessage\nimport operator\n\nclass AgentState(TypedDict):\n    input: str\n    chat_history: list[BaseMessage]\n    agent_outcome: List[dict]\n    intermediate_steps: Annotated[list[tuple[str, str]], operator.add]\n```",
        "testStrategy": "Write a unit test to verify that the `AgentState` dictionary can be instantiated correctly. Test the `operator.add` functionality on the `intermediate_steps` field to ensure it properly concatenates lists when the state is updated.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement `call_model` Node for vLLM Interaction",
        "description": "Create the `call_model` function that interacts with the vLLM. It will construct a prompt including the current user input, chat history, intermediate tool results, and available tool definitions, then parse the LLM's response to update the `agent_outcome` in the state.",
        "details": "Implement a Python function `call_model(state: AgentState)`. This function will:\n1. Fetch tool definitions from the FastAPI-MCP server (can be cached at startup).\n2. Bind the fetched tools to the vLLM instance.\n3. Construct the message list from `state['input']`, `state['chat_history']`, and `state['intermediate_steps']`.\n4. Invoke the vLLM with the messages.\n5. Parse the LLM's response, extracting any tool calls.\n6. Return a dictionary to update the `agent_outcome` field in the `AgentState`.",
        "testStrategy": "Mock the vLLM client. Pass a sample `AgentState` to the function and assert that the messages sent to the mock vLLM are correctly formatted. Verify that a mocked LLM response containing tool calls correctly updates the `agent_outcome` in the returned state dictionary.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement logic to fetch and cache tool definitions",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 2,
            "title": "Create function to construct prompt/message list",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 3,
            "title": "Implement core vLLM client invocation logic",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 4,
            "title": "Develop response parsing logic for tool calls",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 2
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement `call_tool` Node for Single Tool Execution (MVP)",
        "description": "Implement the initial version of the `call_tool` node for the MVP. This node will process the `agent_outcome`, execute a single tool call against its corresponding FastAPI endpoint, and append the result to `intermediate_steps`.",
        "details": "Use LangGraph's `ToolNode` for simplicity. Define wrapper functions for your tools that make HTTP requests to the FastAPI endpoints. The `ToolNode` will automatically handle the invocation based on the tool calls in `agent_outcome`.\n```python\n# In a tools.py file\nimport requests\n\ndef my_single_tool(param1: str) -> str:\n    response = requests.get(f\"http://fastapi-server/tools/my_single_tool?param1={param1}\")\n    response.raise_for_status()\n    return response.json()\n\n# In the main graph definition file\nfrom langgraph.prebuilt import ToolNode\ntools = [my_single_tool]\ntool_node = ToolNode(tools)\n```",
        "testStrategy": "Create a unit test that mocks the FastAPI endpoint using a library like `requests-mock`. Pass a sample `agent_outcome` with a single tool call to the `tool_node`. Verify that the correct mock endpoint is called with the correct parameters and that its return value is correctly formatted and added to `intermediate_steps`.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Implement `should_continue` Conditional Routing Node",
        "description": "Create the conditional logic node that inspects the `agent_outcome` from the `call_model` node to decide the next step in the graph: either execute tools or end the workflow.",
        "details": "Implement a function `should_continue(state: AgentState)` that checks the `agent_outcome` field. If it contains one or more tool calls, the function should return a string that routes the graph to the `call_tool` node (e.g., 'continue'). Otherwise, it should return a string that routes to the `END` state (e.g., 'end').\n```python\ndef should_continue(state: AgentState):\n    if state.get(\"agent_outcome\") and len(state[\"agent_outcome\"]) > 0:\n        return \"continue\"\n    else:\n        return \"end\"\n```",
        "testStrategy": "Unit test the `should_continue` function with two `AgentState` variations: one with `agent_outcome` populated with tool calls, and one where it is empty or None. Assert that the function returns 'continue' and 'end' respectively.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Assemble MVP LangGraph StateMachine",
        "description": "Define the `StateMachine` graph for the MVP. This involves adding the `call_model` and `call_tool` nodes and connecting them with edges based on the `should_continue` conditional logic to create the reasoning loop.",
        "details": "Use `langgraph.graph.StateGraph` to build the workflow. \n1. Set the entry point to the `call_model` node.\n2. Add a conditional edge from `call_model` to `should_continue`.\n3. From `should_continue`, add an edge to `call_tool` for the 'continue' case and an edge to `END` for the 'end' case.\n4. Add an edge from `call_tool` back to `call_model` to complete the loop.\n5. Compile the graph using `workflow.compile()`.",
        "testStrategy": "Perform an integration test on the compiled graph. Mock all external services (vLLM, FastAPI). Invoke the graph with a user input that requires a tool call. Trace the execution path to ensure it flows correctly: `call_model` -> `call_tool` -> `call_model` -> `END`.",
        "priority": "high",
        "dependencies": [
          2,
          3,
          4
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Integrate Orchestrator with Live vLLM and FastAPI Services",
        "description": "Connect the compiled LangGraph orchestrator to the live vLLM server and a running FastAPI server with at least one tool endpoint exposed via FastAPI-MCP. This task validates the end-to-end MVP workflow.",
        "details": "Update the application's configuration to use the actual URLs for the vLLM and FastAPI services. Implement logic to fetch tool definitions from the FastAPI-MCP endpoint at startup. Ensure any required authentication or headers are correctly configured for the HTTP clients.",
        "testStrategy": "Perform an end-to-end test with live services. Send a user query that is known to require the deployed tool. Verify that the vLLM correctly identifies the tool, the orchestrator calls the live FastAPI endpoint, the result is returned to the vLLM, and a final, coherent answer is generated for the user.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement configuration management for live services",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 2,
            "title": "Add startup logic to fetch live tool definitions",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 3,
            "title": "Design and execute end-to-end tests for live services",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          }
        ]
      },
      {
        "id": 7,
        "title": "Enhance `call_tool` for Sequential Multi-Tool Execution",
        "description": "Modify the `call_tool` node to handle an `agent_outcome` containing a list of multiple tool calls, executing them in sequence. This enhancement moves beyond the single-tool MVP.",
        "details": "The default `langgraph.prebuilt.ToolNode` already supports iterating through a list of tool calls and executing them sequentially. The primary work is to ensure the LLM prompt is capable of generating such a list and that the state correctly accumulates the results from each call before looping back to the `call_model` node.",
        "testStrategy": "Create an integration test where the mocked vLLM response contains two or more tool calls. Verify that the `tool_node` executes each tool in the specified order and that all results are correctly appended to the `intermediate_steps` list in the `AgentState`.",
        "priority": "medium",
        "dependencies": [
          3,
          5
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Implement Parallel Tool Execution in `call_tool` Node",
        "description": "Research and implement parallel execution for independent tool calls within the `call_tool` node. This will likely involve using Python's `asyncio` or `concurrent.futures` to improve performance for complex queries.",
        "details": "This may require creating a custom tool-calling node instead of the default `ToolNode`. The custom node will inspect the list of tool calls and use a thread or async pool to execute them concurrently.\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nfrom langgraph.prebuilt import ToolExecutor\n\ntool_executor = ToolExecutor(tools)\n\ndef parallel_tool_node(state: AgentState):\n    tool_calls = state['agent_outcome']\n    with ThreadPoolExecutor() as executor:\n        results = list(executor.map(tool_executor.invoke, tool_calls))\n    return {\"intermediate_steps\": results}\n```\nThe graph definition will need to be updated to use this new `parallel_tool_node`.",
        "testStrategy": "Create a test with two mock tools that have an artificial delay (e.g., `time.sleep(1)`). Invoke the node with calls to both tools. Measure the total execution time to confirm it is significantly less than the sum of the individual delays (e.g., closer to 1 second than 2), proving parallel execution.",
        "priority": "medium",
        "dependencies": [
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement custom `parallel_tool_node` function",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 2,
            "title": "Replace `tool_node` with `parallel_tool_node` in graph",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 3,
            "title": "Develop unit test for `parallel_tool_node`",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 4,
            "title": "Write performance test for parallel execution",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          }
        ]
      },
      {
        "id": 9,
        "title": "Refine LLM Prompting to Encourage Parallelism",
        "description": "Update the system prompt and potentially use few-shot examples to encourage the LLM to identify and request parallelizable tasks in a single turn, maximizing the benefit of the parallel execution capability.",
        "details": "The system prompt should be modified to explicitly instruct the model on its parallel tool-use capabilities. For example: 'You can call multiple tools in a single turn if their operations are independent. For instance, to get the weather in two different cities, request both at the same time.' Few-shot examples demonstrating this syntax should also be included.",
        "testStrategy": "Design a set of user queries where parallel execution is optimal (e.g., 'Compare the weather in New York and Tokyo'). Evaluate the LLM's output to measure the rate at which it correctly generates a list of tool calls for concurrent execution versus generating them sequentially over multiple turns.",
        "priority": "medium",
        "dependencies": [
          8
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Draft and iterate on the new system prompt",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 2,
            "title": "Create a set of high-quality few-shot examples",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 3,
            "title": "Develop an evaluation suite for user queries",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 9
          }
        ]
      },
      {
        "id": 10,
        "title": "Add Error Handling to Tool Execution Node",
        "description": "Implement robust error handling within the tool execution node using `try...except` blocks to gracefully manage failures such as API errors, network issues, or invalid parameters passed by the LLM.",
        "details": "LangGraph's `ToolExecutor` can be used to automatically catch exceptions during tool execution. When an exception occurs, it returns a `ToolException` object. The node's logic should ensure this exception is caught and formatted into a user-friendly error message string.",
        "testStrategy": "Unit test the tool execution logic by mocking a tool to raise specific exceptions (e.g., `requests.HTTPError`, `ValueError`). Verify that the node catches the exception and returns a formatted error string in the `intermediate_steps` instead of crashing the graph.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "Implement Error Feedback Loop to LLM",
        "description": "When a tool fails, ensure the formatted error message is appended to `intermediate_steps`. This provides the `call_model` node with the error context, allowing the LLM to correct its action or inform the user.",
        "details": "This task is a direct follow-on from implementing error handling. The `call_model` node must be confirmed to correctly include the error messages from `intermediate_steps` in the subsequent prompt to the LLM. The system prompt should guide the LLM on how to interpret and react to these tool errors.",
        "testStrategy": "Run an end-to-end test where a tool call is designed to fail (e.g., calling an API with invalid data). Trace the graph's execution. Confirm that the error is captured, passed back to the `call_model` node, and that the subsequent LLM call includes the error message in its context, allowing the LLM to attempt a recovery.",
        "priority": "high",
        "dependencies": [
          10
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Integrate Stateful Conversation History",
        "description": "Integrate a memory solution like `langchain.memory.ConversationBufferWindowMemory` to manage the `chat_history` field in `AgentState`, providing essential context for multi-turn dialogues.",
        "details": "Instantiate `ConversationBufferWindowMemory` in the main application logic. Before invoking the graph, load the history for the current conversation into the initial `AgentState`. After the graph execution completes, save the new user input and the agent's final response back into the memory object.\n```python\n# In main application logic\nmemory = ConversationBufferWindowMemory(k=10, return_messages=True)\n\ndef process_request(input_text, conversation_id):\n    state = {\"input\": input_text, \"chat_history\": memory.load_memory_variables({})['history'], ...}\n    final_state = app.invoke(state)\n    memory.save_context(...)\n```",
        "testStrategy": "Create a multi-turn conversation test script. Turn 1: 'What is the capital of France?'. Turn 2: 'What is its population?'. Verify that for the second turn, the agent correctly infers 'it' refers to Paris by using the context from `chat_history`. Check that the history is correctly populated and truncated by the window size.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement `ConversationBufferWindowMemory` management",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 12
          },
          {
            "id": 2,
            "title": "Modify request handler to load history into `AgentState`",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 12
          },
          {
            "id": 3,
            "title": "Modify request handler to save new interaction to memory",
            "description": "",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 12
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-06-28T22:47:14.740Z",
      "updated": "2025-07-02T00:47:45.812Z",
      "description": "Tasks for master context"
    }
  }
}