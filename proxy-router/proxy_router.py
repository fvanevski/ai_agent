# proxy_router.py ─ unified GPU router / gatekeeper
import threading, json, httpx, os
from dotenv import load_dotenv
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from supervisor import (
    record_activity, run_supervisor, is_model_awake,
    wake_model
)
from typing import TypedDict, Annotated, List, Dict
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
import operator
from langgraph.prebuilt import ToolNode
from langgraph.graph import StateGraph, END
from tools import search_file_content
from langchain.memory import ConversationBufferWindowMemory

load_dotenv()

app = FastAPI()

# CORS configuration
origins = [
    "http://localhost:3000",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# GPU-bound services and their local ports
MODEL_PORTS = {
    "llama.cpp-embedding": 8001,
    "vllm-agent":          8002,
    "faster-whisper":      8003,
}

# ───────────────────────────────────────────────────────────────
# Path  →  target-service map
#   (exact match is enough because we keep the endpoints short)
# ───────────────────────────────────────────────────────────────
MODEL_BY_PATH = {
    "/v1/embeddings":          "llama.cpp-embedding",
    "/v1/chat/completions":    "vllm-agent",

    # Faster-Whisper endpoints
    "/transcribe":             "faster-whisper",
    "/transcribe_url":         "faster-whisper",
    # vllm-agent wake endpoint
    "/wake_up":                "vllm-agent",
}

TIMEOUT = float(os.getenv("PROXY_TIMEOUT", 1800))

# Global cache for tool definitions
TOOL_DEFINITIONS_CACHE = {}

def fetch_and_cache_tool_definitions():
    """
    Fetches tool definitions from the tools-api service and caches them.
    """
    global TOOL_DEFINITIONS_CACHE
    tools_api_url = os.getenv("TOOLS_API_URL", "http://localhost:9000")
    try:
        with httpx.Client() as client:
            response = client.get(f"{tools_api_url}/get_tools")
            response.raise_for_status()
            TOOL_DEFINITIONS_CACHE = response.json()
            print(f"Successfully fetched and cached tool definitions.")
            # Optional: print cached tools for debugging
            # import json
            # print(json.dumps(TOOL_DEFINITIONS_CACHE, indent=2))

    except httpx.RequestError as e:
        print(f"Error fetching tool definitions from tools-api: {e}")
        TOOL_DEFINITIONS_CACHE = {"langgraph": [], "mcpo": []}
    except Exception as e:
        print(f"An unexpected error occurred while fetching tools: {e}")
        TOOL_DEFINITIONS_CACHE = {"langgraph": [], "mcpo": []}



# In-memory store for conversation history
conversation_store: Dict[str, ConversationBufferWindowMemory] = {}

class AgentState(TypedDict):
    # A unique identifier for the conversation
    conversation_id: str
    # The user's input
    input: str
    # The enabled tools, separated by type
    enabled_tools: Dict[str, List[str]]
    # The conversation history
    chat_history: list[BaseMessage]
    # The list of tool calls generated by the LLM
    agent_outcome: List[dict]
    # The results of the tool executions
    intermediate_steps: Annotated[list[ToolMessage], operator.add]

def construct_message_list(state: AgentState) -> list:
    """
    Constructs the list of messages to be sent to the LLM.
    """
    messages = []
    # Add the chat history
    for msg in state.get("chat_history", []):
        if isinstance(msg, HumanMessage):
            messages.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            messages.append({"role": "assistant", "content": msg.content})
        elif isinstance(msg, ToolMessage):
             messages.append({
                "role": "tool",
                "content": msg.content,
                "tool_call_id": msg.tool_call_id
            })

    # Add the user's current input
    messages.append({"role": "user", "content": state["input"]})

    # Add the results of any intermediate tool executions from the current turn
    if "intermediate_steps" in state:
        for tool_message in state["intermediate_steps"]:
            messages.append({
                "role": "tool",
                "content": tool_message.content,
                "tool_call_id": tool_message.tool_call_id
            })
            
    return messages


def call_model(state: AgentState) -> dict:
    """
    Invokes the vLLM with the current state and returns the LLM's response.
    """
    conversation_id = state["conversation_id"]
    if conversation_id not in conversation_store:
        conversation_store[conversation_id] = ConversationBufferWindowMemory(k=10, return_messages=True)

    memory = conversation_store[conversation_id]
    
    # Load history into state
    state["chat_history"] = memory.load_memory_variables({})['history']

    messages = construct_message_list(state)
    
    vllm_url = "http://localhost:8002/v1/chat/completions"
    
    enabled_tool_config = state.get("enabled_tools", {})
    enabled_lg_tools = enabled_tool_config.get("langgraph", [])
    enabled_mcpo_servers = enabled_tool_config.get("mcpo", [])

    available_tools = []

    # Add enabled LangGraph tools
    for tool in TOOL_DEFINITIONS_CACHE.get("langgraph", []):
        if tool["name"] in enabled_lg_tools:
            available_tools.append({
                "type": "function",
                "function": {
                    "name": tool["name"],
                    "description": tool["description"],
                    "parameters": tool["parameters"]
                }
            })

    # Add tools from enabled MCPO servers
    for server in TOOL_DEFINITIONS_CACHE.get("mcpo", []):
        if server["name"] in enabled_mcpo_servers:
            for tool in server["tools"]:
                available_tools.append({
                    "type": "function",
                    "function": {
                        "name": tool["name"],
                        "description": tool["description"],
                        "parameters": tool["parameters"]
                    }
                })

    with httpx.Client() as client:
        print(f"Sending messages to vLLM: {messages}")
        print(f"Sending {len(available_tools)} tools to vLLM: {[t['function']['name'] for t in available_tools]}")
        response = client.post(
            vllm_url,
            json={"messages": messages, "tools": available_tools, "tool_choice": "auto"},
            headers={"Content-Type": "application/json"}
        )
        response.raise_for_status()
        llm_response = response.json()

    # Save context
    # The input to save is the user's last message
    user_input = state["input"]
    # The output to save is the LLM's response content
    llm_content = llm_response.get("choices", [{}])[0].get("message", {}).get("content")
    # Also save any tool calls made by the assistant
    tool_calls = llm_response.get("choices", [{}])[0].get("message", {}).get("tool_calls")

    # Construct the full assistant message for memory
    assistant_message_for_memory = AIMessage(content=llm_content or "", tool_calls=tool_calls or [])
    
    memory.save_context({"input": HumanMessage(content=user_input)}, {"output": assistant_message_for_memory})
    
    # The agent_outcome should be the raw response from the LLM
    return {"agent_outcome": [llm_response]}

def should_continue(state: AgentState) -> str:
    """
    Determines whether to continue with tool calls or stop.
    """
    if "agent_outcome" not in state or not state["agent_outcome"]:
        return "end"
    
    # Check the last message from the LLM for tool calls
    last_llm_response = state["agent_outcome"][-1]
    if last_llm_response.get("choices", [{}])[0].get("message", {}).get("tool_calls"):
        return "action"  # Changed from "continue" to match the graph edge
    
    return "end"


def call_tool_node(state: AgentState) -> dict:
    """
    Executes tools by calling the tools-api microservice.
    """
    tool_messages = []
    agent_outcome = state.get("agent_outcome")
    if not agent_outcome:
        return {"intermediate_steps": []}

    # The LLM's response is in the last element of agent_outcome
    llm_response = agent_outcome[-1]
    tool_calls = llm_response.get("choices", [{}])[0].get("message", {}).get("tool_calls", [])

    if not tool_calls:
        return {"intermediate_steps": []}

    tools_api_url = os.getenv("TOOLS_API_URL", "http://localhost:9000")

    for tool_call in tool_calls:
        tool_name = tool_call.get("function", {}).get("name")
        tool_args_str = tool_call.get("function", {}).get("arguments", "{}")
        tool_call_id = tool_call.get("id")
        
        try:
            tool_args = json.loads(tool_args_str)
        except json.JSONDecodeError:
            # If args are not valid JSON, treat as an error
            error_msg = f"Error: Invalid JSON arguments for tool {tool_name}"
            tool_messages.append(ToolMessage(content=error_msg, tool_call_id=tool_call_id))
            continue

        if not tool_name:
            continue

        run_tool_payload = {"tool_name": tool_name, "args": tool_args}
        
        try:
            with httpx.Client() as client:
                response = client.post(f"{tools_api_url}/run_tool", json=run_tool_payload, timeout=120)
                response.raise_for_status()
                tool_result = response.json()
                # We wrap the JSON result in a string for the ToolMessage
                tool_messages.append(ToolMessage(content=json.dumps(tool_result), tool_call_id=tool_call_id))

        except httpx.RequestError as e:
            error_msg = f"Error calling tools-api for {tool_name}: {e}"
            tool_messages.append(ToolMessage(content=error_msg, tool_call_id=tool_call_id))
        except Exception as e:
            error_msg = f"An unexpected error occurred while running tool {tool_name}: {e}"
            tool_messages.append(ToolMessage(content=error_msg, tool_call_id=tool_call_id))

    return {"intermediate_steps": tool_messages}

def create_graph():
    """
    Creates the agent graph.
    """
    workflow = StateGraph(AgentState)

    workflow.add_node("agent", call_model)
    workflow.add_node("action", call_tool_node)

    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "action": "action",
            "end": END,
        },
    )

    workflow.add_edge("action", "agent")

    workflow.set_entry_point("agent")

    return workflow.compile()

app = FastAPI()




# Global variable to hold the compiled graph
fetch_and_cache_tool_definitions()
app_graph = create_graph() # Compile the graph once at startup

@app.get("/health")
async def health_check():
    return {"status": "ok"}

@app.post("/chat")
async def chat_endpoint(request: Request):
    try:
        print("Request received at /chat endpoint")
        body = await request.json()
        conversation_id = body.get("conversation_id", "default")
        user_input = body.get("input")
        messages = body.get("messages", [])
        enabled_tools = body.get("enabled_tools", {"langgraph": [], "mcpo": []})

        if not user_input:
            return JSONResponse(status_code=400, content={"error": "Input cannot be empty"})

        # Create the initial state for the graph
        initial_state = AgentState(
            conversation_id=conversation_id,
            input=user_input,
            enabled_tools=enabled_tools,
            chat_history=messages,
            agent_outcome=[],
            intermediate_steps=[]
        )

        # Compile the graph if not already compiled
        global app_graph
        if app_graph is None:
            app_graph = create_graph()

        # Invoke the graph
        final_state = app_graph.invoke(initial_state)
        print(f"Final state after graph invocation: {final_state}")

        # Extract the final answer from the agent_outcome
        final_answer = ""
        if final_state and final_state.get("agent_outcome"):
            last_outcome = final_state["agent_outcome"][-1]
            if "choices" in last_outcome and last_outcome["choices"]:
                message = last_outcome["choices"][0].get("message", {})
                final_answer = message.get("content")
                # If there's no content but there are tool calls, you might want to format a message
                if not final_answer and message.get("tool_calls"):
                    final_answer = f" টুল কল করা হয়েছে: {[tc['function']['name'] for tc in message['tool_calls']]}"
            elif "content" in last_outcome:
                final_answer = last_outcome["content"]

        return JSONResponse(content={"conversation_id": conversation_id, "response": final_answer})
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": f"An error occurred: {e}"})


if __name__ == "__main__":
    threading.Thread(target=run_supervisor, daemon=True).start()
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
